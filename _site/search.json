[
  {
    "objectID": "posts/02_Medical_Costs_Regression/Medical_Costs_Regression.html#key-results",
    "href": "posts/02_Medical_Costs_Regression/Medical_Costs_Regression.html#key-results",
    "title": "Predicting Medical Costs - A Regression Problem",
    "section": "1. Key Results",
    "text": "1. Key Results\n\nCharges generally rise with age.\nSmokers consistently have higher and more variable charges than non-smokers.\n\n\n\n\nFig. 0 Showing the most important features identified by a Decision Tree Regressor.\n\n\n\n\n\n\n\n\n\n\n\nModels\nMAE\nMSE\nRMSE\nR2\n\n\n\n\n0\nLinearRegression\n4092.36\n31333268.50\n5597.61\n0.79\n\n\n1\nSVR\n3373.30\n37706594.68\n6140.57\n0.75\n\n\n2\nDecisionTreeRegressor\n2755.59\n21655894.86\n4653.59\n0.86\n\n\n3\nRandomForestRegressor\n2459.69\n18958974.55\n4354.19\n0.88"
  },
  {
    "objectID": "posts/02_Medical_Costs_Regression/Medical_Costs_Regression.html#why-it-matters",
    "href": "posts/02_Medical_Costs_Regression/Medical_Costs_Regression.html#why-it-matters",
    "title": "Predicting Medical Costs - A Regression Problem",
    "section": "2. Why It Matters",
    "text": "2. Why It Matters\nPredicting medical costs isn’t just a math problem — it’s about understanding the real price of our health choices. For insurers, these predictions guide fairer pricing and risk assessment. For individuals, they reveal how habits like smoking or maintaining a healthy BMI can shape future expenses. In a world where healthcare costs can make or break financial stability, this kind of model helps turn data into smarter, more informed decisions."
  },
  {
    "objectID": "posts/02_Medical_Costs_Regression/Medical_Costs_Regression.html#the-data",
    "href": "posts/02_Medical_Costs_Regression/Medical_Costs_Regression.html#the-data",
    "title": "Predicting Medical Costs - A Regression Problem",
    "section": "3. The Data",
    "text": "3. The Data\nThe dataset comes from the Medical Cost Personal Dataset on Kaggle, containing 1,338 observations, each representing a single U.S. health insurance policyholder. Key features include age, sex, BMI, number of children, smoker status, and region. The target variable, charges, captures each person’s annual medical cost, ranging from about $1,000 to over $60,000. Despite its small size, the dataset offers rich insights into how lifestyle and demographics drive healthcare expenses.\n\n\n\nFig. 1 Shows the Distribution of Numeric Features in the Medical Costs Dataset\n\n\n\n\n\nFig. 2 Shows the Distribution of Categorical Features in the Medical Costs Dataset"
  },
  {
    "objectID": "posts/02_Medical_Costs_Regression/Medical_Costs_Regression.html#what-i-did",
    "href": "posts/02_Medical_Costs_Regression/Medical_Costs_Regression.html#what-i-did",
    "title": "Predicting Medical Costs - A Regression Problem",
    "section": "4. What I Did",
    "text": "4. What I Did\nExplored the medical cost dataset with visuals — distributions, correlations, and feature effects — to link age, BMI, and smoking to charges. Cleaned numeric features with StandardScaler and categorical variables with OneHotEncoder. Trained LinearRegression, SVR, DecisionTreeRegressor, and RandomForestRegressor, tuning hyperparameters with RandomizedSearchCV. Evaluated MAE, MSE, RMSE, and R² on the test set and compared models visually."
  },
  {
    "objectID": "posts/02_Medical_Costs_Regression/Medical_Costs_Regression.html#what-i-found",
    "href": "posts/02_Medical_Costs_Regression/Medical_Costs_Regression.html#what-i-found",
    "title": "Predicting Medical Costs - A Regression Problem",
    "section": "5. What I Found",
    "text": "5. What I Found\nInsurance charges tend to increase with age. Smokers have significantly higher and more variable charges compared to non-smokers, whose costs show a smoother, more consistent trend.\nAmong smokers, insurance charges generally increase with higher BMI, while BMI has little impact on costs for non-smokers. Within non-smokers, age drives costs.\nFeature importance analysis (from the tree-based models) highlighted smoking, age, and BMI as the top predictors — confirming both domain expectations and model consistency.\nThe Random Forest Regressor achieved the best results, with the lowest RMSE (≈ 4354) and highest R² (≈ 0.88), indicating strong predictive accuracy and generalization on unseen data, showing that it is possible to predict medical costs based on key personal and lifestyle factors. This demonstrates that data-driven models can effectively capture the underlying relationships between health behaviors and medical expenses, enabling more accurate cost forecasting and better-informed healthcare and policy decisions.\n\n\n\nFig. 3 Charges increase with age, and smokers have significantly higher and more variable charges than non-smokers.\n\n\n\n\n\nFig. 4 Smokers consistently have higher charges than non-smokers. Among smokers, charges increase strongly with BMI (r ≈ 0.81), while among non-smokers, BMI has almost no relationship with charges (r ≈ 0.08). This indicates a strong interaction effect — smoking amplifies the impact of BMI on charges.\n\n\n\n\n\nFig. 5 Decision tree regression model predicting medical insurance charges. The model splits data points into groups based on key factors. The most important splitting criterion is smoking status. Among non-smokers, age is the most influential variable, while for smokers, BMI plays the dominant role. The “value” shown in each node represents the model’s predicted target value (here, the estimated insurance cost in dollars).\n\n\n\n\n\nFig. 6 Comparison of Root Mean Squared Error (RMSE) values for different regression models on the test set. The RMSE measures the average magnitude of prediction errors, with lower values indicating better model performance. The Random Forest Regressor achieved the lowest RMSE, followed by the Decision Tree Regressor, while the SVR and Linear Regression models showed higher error levels, suggesting less accurate predictions.)\n\n\n\n\n\nFig. 7 Comparison of R² scores for different regression models on the test set. The R² score represents the proportion of variance in medical insurance charges explained by each model. Higher values indicate better predictive performance. The Random Forest Regressor achieved the highest R², followed closely by the Decision Tree Regressor, while Linear Regression and SVR showed comparatively lower explanatory power."
  },
  {
    "objectID": "posts/02_Medical_Costs_Regression/Medical_Costs_Regression.html#what-i-learned",
    "href": "posts/02_Medical_Costs_Regression/Medical_Costs_Regression.html#what-i-learned",
    "title": "Predicting Medical Costs - A Regression Problem",
    "section": "6. What I Learned",
    "text": "6. What I Learned\nModel performance improved with complexity:\nTree-based models, especially Random Forest, captured the nonlinear impact of health and lifestyle factors most effectively. The findings align with real-world intuition — smokers and older individuals face substantially higher insurance costs."
  },
  {
    "objectID": "posts/02_Medical_Costs_Regression/Medical_Costs_Regression.html#whats-next",
    "href": "posts/02_Medical_Costs_Regression/Medical_Costs_Regression.html#whats-next",
    "title": "Predicting Medical Costs - A Regression Problem",
    "section": "7. What’s Next",
    "text": "7. What’s Next\nThis project shows that predicting medical costs is possible — but there’s room to grow. Future work could explore more advanced models like Gradient Boosting or XGBoost to see if they capture even subtler patterns in the data. Another direction is adding richer features to improve model depth and realism. Finally, scaling this work to larger, more diverse datasets would help assess how well these findings hold up across different populations. The next steps are open — it’s up to you to take this foundation and push it further."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Topics",
    "section": "",
    "text": "Hello Blog!\n\n\n\n\n\n\n\n\nOct 25, 2025\n\n\nPatrick Linke\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\nWho Survived the Titanic? A Data-Driven Look at Chance, Class, and Choice\n\n\n\nClassification\n\n\n\n\n\n\n\n\n\nNov 2, 2025\n\n\nPatrick Linke\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Medical Costs - A Regression Problem\n\n\n\nRegression\n\nMedicine\n\n\n\n\n\n\n\n\n\nOct 27, 2025\n\n\nPatrick Linke\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "contact.html#lets-connect",
    "href": "contact.html#lets-connect",
    "title": "Contact",
    "section": "Let’s Connect",
    "text": "Let’s Connect\nI’d love to connect with fellow machine learning enthusiasts, especially those interested in healthcare applications and data science.\n\nLinkedIn: My LinkedIn Profile\nGitHub: My GitHub Profile\n\n\nAlways happy to chat about ML, medicine, and the intersection of both!"
  },
  {
    "objectID": "about.html#who-am-i",
    "href": "about.html#who-am-i",
    "title": "About Me",
    "section": "Who am I?",
    "text": "Who am I?\nHi, I’m Patrick — a medical student fascinated by how machine learning and artificial intelligence are reshaping healthcare.\nThis blog is where I document my journey — learning, experimenting, and sharing insights as I explore how data and algorithms can improve decision-making."
  },
  {
    "objectID": "about.html#why-this-blog-exists",
    "href": "about.html#why-this-blog-exists",
    "title": "About Me",
    "section": "Why This Blog Exists",
    "text": "Why This Blog Exists\nThis blog represents my commitment to learning in public—documenting my journey as I build hands-on projects and deepen my understanding of machine learning. Here’s what drives me:\n\nBridging domains: Exploring how machine learning can enhance medical research and clinical practice\nBuilding intuition: Working through real datasets to understand algorithms beyond theory\nSharing knowledge: Helping fellow learners see practical workflows and honest reflections\nContinuous growth: Documenting failures and successes as part of the learning process"
  },
  {
    "objectID": "about.html#what-youll-find-here",
    "href": "about.html#what-youll-find-here",
    "title": "About Me",
    "section": "What You’ll Find Here",
    "text": "What You’ll Find Here\nThis isn’t a tutorial site — it’s a documentation of my learning process. Each post represents a step in my understanding of machine learning, from data preprocessing to model evaluation and interpretation.\nCurrent Focus Areas:\n\nMedical data analysis and prediction\nClassical machine learning algorithms\nFeature engineering and selection\nModel interpretation and validation"
  },
  {
    "objectID": "about.html#my-approach",
    "href": "about.html#my-approach",
    "title": "About Me",
    "section": "My Approach",
    "text": "My Approach\nI focus on:\n\nPractical projects with real-world datasets\nClear explanations of methods and reasoning\nVisual storytelling through charts and analysis\nReproducible code that others can learn from\nHonest documentation of challenges and insights"
  },
  {
    "objectID": "about.html#start-here",
    "href": "about.html#start-here",
    "title": "About Me",
    "section": "Start Here",
    "text": "Start Here\nYou can start here or explore the Blog! Have fun!"
  },
  {
    "objectID": "about.html#lets-connect",
    "href": "about.html#lets-connect",
    "title": "About Me",
    "section": "Let’s Connect",
    "text": "Let’s Connect\nI’d love to connect with fellow machine learning enthusiasts, especially those interested in healthcare applications:\n\nLinkedIn: My LinkedIn Profile\nGitHub: My GitHub Profile\n\n\nLearning in public. Bridging medicine and machine learning. Sharing the journey one project at a time."
  },
  {
    "objectID": "home.html",
    "href": "home.html",
    "title": "Intelligent Machines",
    "section": "",
    "text": "Getting Started\n\n\n\nCheck out my latest projects in the Blog section, where I share complete workflows, code, and insights from working with real datasets.\n\n\n\nWelcome to my learning-in-public journal where I document my journey through applied machine learning. Here, you’ll find more information about this blog.\n\nLearning in public • Building understanding • Sharing insights"
  },
  {
    "objectID": "home.html#recent-posts",
    "href": "home.html#recent-posts",
    "title": "Intelligent Machines",
    "section": "",
    "text": "Check out my latest projects in the Blog section, where I share complete workflows, code, and insights from working with real datasets."
  },
  {
    "objectID": "home.html#exploring-machine-learning-through-real-data",
    "href": "home.html#exploring-machine-learning-through-real-data",
    "title": "Intelligent Machines",
    "section": "",
    "text": "Welcome to my learning-in-public journal where I document my journey through applied machine learning. Here, you’ll find more information about this blog.\n\nLearning in public • Building understanding • Sharing insights"
  },
  {
    "objectID": "posts/01_Welcome/Hello_Blog.html",
    "href": "posts/01_Welcome/Hello_Blog.html",
    "title": "Hello Blog!",
    "section": "",
    "text": "Hello and Welcome to My Blog!\nI decided to start this blog to document my machine learning journey in public.\nI am fascinated by idea of building intelligent machines that can learn from data and help make better decisions - especially in fields where precision matters and the cost of errors is high.\nI’m a medical student, so the content will often feature medical topics. But beyond that, I’ll explore whatever I find interesting and intriguing at the moment and share what I learn along the way.\n\n\nJust Read Along\nNot everyone wants — or needs — to write code, and that’s perfectly fine.\nYou can still enjoy this blog by reading along, exploring the visuals, and following my explanations.\nThe goal is to make complex ideas intuitive, whether you’re coding beside me or just curious about how it all works.\n\n\nHow You Can Code Along\nIf you’d like to try the code yourself, you can download my notebooks and run the directly in Google Colab or on Kaggle — no installation required!\nOf course, you can also type along without downloading the notebooks.\nI highly encourage you to experiment with the code — this is one of the best learning experiences you’ll have!\nIf you get stuck or have any questions, feel free to reach out to my via the Contact page or ask an LLM :).\nIn general, ChatGPT, Claude, Gemnini, and similar models are great at explaining code, so feel free to use them to enhance your learning experience!\nThat said, there’s no pressure to code along. You can absolutely enjoy this blog by simply reading the posts, exploring the visuals, and following the explanations.\nEven without writing code, you’ll still get insights into how machine learning works in practice.\n\n\nProgramming Language and Libraries\nThe code will be written in Python, a programming language that’s been around since 1991.\nPython is great for data science, statistics, automation, machine learning and AI. It makes it easy to build prototypes and MVPs quickly. It is also fairly easy to learn, so I encourage you to check it out if any of the topics sound interesting.\nCore libraries I’ll use:\n\nNumpy — fundamental package for scientific computing\n\nPandas — data analysis and manipulation of tabular data\n\nMatplotlib.pyplot — plotting figures and creating visualizations\n\nSeaborn — advanced statistical plotting\nScikit-Learn — classical machine learning algorithms\n\n\n\nThanks for Reading This First Post!\nIf you’d like to know more about me and why I started this blog, check out the About page. Otherwise, dive into the next post and have fun!\nThis is just the beginning. Let’s see where curiosity — and a bit of code — takes us.\n\n\nCredits\nP.S. The image at the beginning of this post was inspired by @EdDonner’s fantastic course on AI Agents!\nWhile the style was inspired by the course, I created the final image myself using an AI model based on my own instructions."
  },
  {
    "objectID": "posts/03_titanic/Titanic_blog.html",
    "href": "posts/03_titanic/Titanic_blog.html",
    "title": "Who Survived the Titanic? A Data-Driven Look at Chance, Class, and Choice",
    "section": "",
    "text": "PREVIEW IMAGE"
  },
  {
    "objectID": "posts/03_titanic/Titanic_blog.html#key-results",
    "href": "posts/03_titanic/Titanic_blog.html#key-results",
    "title": "Who Survived the Titanic? A Data-Driven Look at Chance, Class, and Choice",
    "section": "1 Key Results",
    "text": "1 Key Results\nWho survived the Titanic shipwreck — and why? Beneath this tragic event lies a fascinating data story of class, age, and gender.\nThrough Exploratory Data Analysis (EDA) and machine learning, I built models to predict passenger survival and uncover the hidden patterns behind the numbers. While the overall survival rate was only about 38%, survival wasn’t random — women, children, and first-class passengers had dramatically higher chances. This project became a hands-on introduction to classification modeling and key evaluation metrics like Precision, Recall, and F1-Score.\n\n\n\nFig. 1 Shows the Accuracy scores of different Baseline Models, along with a box plot highlighting the lower and upper quartiles, and “whiskers” showing the extent of the scores. The SVM was then selected for fine-tuning and evaluating Performance Metrics."
  },
  {
    "objectID": "posts/03_titanic/Titanic_blog.html#why-it-matters",
    "href": "posts/03_titanic/Titanic_blog.html#why-it-matters",
    "title": "Who Survived the Titanic? A Data-Driven Look at Chance, Class, and Choice",
    "section": "2. Why It Matters",
    "text": "2. Why It Matters\nPredicting survival in the Titanic disaster dataset isn’t just an academic challenge — it’s central to how we think about risk, fairness and decision-making in real-world scenarios. When we study what factors (such as gender, class, age) determined survival, these insights help policy makers, emergency planners and data scientists to better design protocols that save lives in future crises.\nBy applying Exploratory Data Analysis (EDA) and machine learning techniques to this classification problem, we learn not only how to build models that predict outcomes, but also why those outcomes occurred. That matters because in the fields of transportation safety, disaster response and social equity, it’s not enough to know who survived — we want to understand why. These learnings help organisations and analysts make informed decisions around resource allocation, training and evacuation strategies — ultimately leading to safer systems and more equitable responses."
  },
  {
    "objectID": "posts/03_titanic/Titanic_blog.html#the-data",
    "href": "posts/03_titanic/Titanic_blog.html#the-data",
    "title": "Who Survived the Titanic? A Data-Driven Look at Chance, Class, and Choice",
    "section": "3. The Data",
    "text": "3. The Data\nThe dataset from the Kaggle competition “Titanic – Machine Learning from Disaster” consists of a training set of 891 passenger records and a test set of 418 records. Each row describes a passenger on the RMS Titanic and includes features such as:\n\nTicket class: 1st, 2nd, 3rd\nSex (male or female)\nAge (in years)\nNumber of siblings/spouses aboard\nNumber of parents/children aboard\nTicket fare paid\nPort of embarkation: Cherbourg/Queenstown/Southampton\n\nThe target variable is Survived (0 = died, 1 = survived) for each passenger in the training set.\nLink to the public dataset: https://www.kaggle.com/competitions/titanic/data\n\n\n\nFig. 2 Shows the Distribution of the Features in the Dataset. Port of Embarkation: C = Cherbourg, Q = Queenstown, S = Southampton\n\n\n\n\n\nFig. 3 Distribution of the Target. Overall Survival Rate is ~38%"
  },
  {
    "objectID": "posts/03_titanic/Titanic_blog.html#what-i-did",
    "href": "posts/03_titanic/Titanic_blog.html#what-i-did",
    "title": "Who Survived the Titanic? A Data-Driven Look at Chance, Class, and Choice",
    "section": "4. What I Did",
    "text": "4. What I Did\nThe workflow followed a straightforward pattern: EDA → Feature Engineering → Model Training → Evaluation.\nDuring feature engineering, I created meaningful variables such as AgeGroup (Child, Teen, Young Adult, Adult, Senior), cabin_present (indicating whether a cabin number was available), and RelativesOnboard (the sum of SibSp and Parch).\nI trained several baseline models — SGD, SVM, Decision Tree, and Random Forest — and compared their performance using accuracy. The SVM model performed best, so I fine-tuned it with GridSearchCV to optimize hyperparameters.\nFinal performance was evaluated using Accuracy, Precision, Recall, and F1-Score to ensure balanced assessment across all prediction outcomes."
  },
  {
    "objectID": "posts/03_titanic/Titanic_blog.html#what-i-found",
    "href": "posts/03_titanic/Titanic_blog.html#what-i-found",
    "title": "Who Survived the Titanic? A Data-Driven Look at Chance, Class, and Choice",
    "section": "5. What I Found",
    "text": "5. What I Found\nInfluence of Features on Survival Rates\n\nGender was the single strongest predictor of survival. Females had consistently higher survival probabilities (~0.59–0.94), while males saw a steep drop after childhood (~0.09–0.23).\nPassenger Class clearly stratified outcomes — higher class meant better chances. The extremes: 1st-class females (~0.97 survival) vs 3rd-class males (~0.14).\nAge mattered: children were prioritized during evacuation. Boys and girls under 12 had similar survival rates (~0.57–0.59), but after adolescence, the gender gap widened sharply.\n\n\n\n\nFig. 4 Survival Rate by Passenger Class and Gender. Gender dominates survival: females outlive males in every class. Class amplifies it: higher class → higher survival for both genders.\n\n\n\n\n\nFig. 5 Survival Rate by Age and Gender. Clear “women and children first” pattern; gender dominates age once past childhood. Strong gender effect: females had high survival across ages (~0.59–0.94), males low after childhood (~0.09–0.23). Children were prioritized: boys and girls had similar, relatively high survival.\n\n\nPerformance Metrics of the Best Model (SVM): - Accuracy 82% - Precision: 80% - Recall: 72% - F1-Score: 76%\n\n\n\nFig. 6 Precision-Recall Curve of the SVM Model.\n\n\n\n\n\nFig. 7 ROC Curve of the SVM Model. AUC = 0.83."
  },
  {
    "objectID": "posts/03_titanic/Titanic_blog.html#what-i-learned",
    "href": "posts/03_titanic/Titanic_blog.html#what-i-learned",
    "title": "Who Survived the Titanic? A Data-Driven Look at Chance, Class, and Choice",
    "section": "6. What I Learned",
    "text": "6. What I Learned\nThis project taught me how to build and evaluate Classification Models end to end — from preprocessing raw data to fine-tuning hyperparameters. I deepened my understanding of key evaluation metrics such as Accuracy, Precision, Recall, F1-Score, and the Confusion Matrix, as well as diagnostic tools like the Precision-Recall and ROC Curves.\nThrough EDA and thoughtful feature engineering, I saw how data visualization and preprocessing pipelines can transform raw numbers into insights that explain why models make certain predictions — not just how well they perform."
  }
]