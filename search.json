[
  {
    "objectID": "posts/02_Medical_Costs_Regression/blog_Regression_project.html#key-results",
    "href": "posts/02_Medical_Costs_Regression/blog_Regression_project.html#key-results",
    "title": "Predicting Medical Costs - A Regression Problem",
    "section": "1. Key Results",
    "text": "1. Key Results\n\nCharges generally rise with age.\nSmokers consistently have higher and more variable charges than non-smokers.\n\n\n\n\nFig. 0 Showing the most important features identified by a Decision Tree Regressor.\n\n\n\n\n\n\n\n\n\n\n\nModels\nMAE\nMSE\nRMSE\nR2\n\n\n\n\n0\nLinearRegression\n4092.36\n31333268.50\n5597.61\n0.79\n\n\n1\nSVR\n3373.30\n37706594.68\n6140.57\n0.75\n\n\n2\nDecisionTreeRegressor\n2755.59\n21655894.86\n4653.59\n0.86\n\n\n3\nRandomForestRegressor\n2459.69\n18958974.55\n4354.19\n0.88"
  },
  {
    "objectID": "posts/02_Medical_Costs_Regression/blog_Regression_project.html#why-it-matters",
    "href": "posts/02_Medical_Costs_Regression/blog_Regression_project.html#why-it-matters",
    "title": "Predicting Medical Costs - A Regression Problem",
    "section": "2. Why It Matters",
    "text": "2. Why It Matters\nPredicting medical costs isn’t just a math problem — it’s about understanding the real price of our health choices. For insurers, these predictions guide fairer pricing and risk assessment. For individuals, they reveal how habits like smoking or maintaining a healthy BMI can shape future expenses. In a world where healthcare costs can make or break financial stability, this kind of model helps turn data into smarter, more informed decisions."
  },
  {
    "objectID": "posts/02_Medical_Costs_Regression/blog_Regression_project.html#the-data",
    "href": "posts/02_Medical_Costs_Regression/blog_Regression_project.html#the-data",
    "title": "Predicting Medical Costs - A Regression Problem",
    "section": "3. The Data",
    "text": "3. The Data\nThe dataset comes from the Medical Cost Personal Dataset on Kaggle, containing 1,338 observations, each representing a single U.S. health insurance policyholder. Key features include age, sex, BMI, number of children, smoker status, and region. The target variable, charges, captures each person’s annual medical cost, ranging from about $1,000 to over $60,000. Despite its small size, the dataset offers rich insights into how lifestyle and demographics drive healthcare expenses.\n\n\n\nFig. 1 Shows the Distribution of Numeric Features in the Medical Costs Dataset\n\n\n\n\n\nFig. 2 Shows the Distribution of Categorical Features in the Medical Costs Dataset"
  },
  {
    "objectID": "posts/02_Medical_Costs_Regression/blog_Regression_project.html#what-i-did",
    "href": "posts/02_Medical_Costs_Regression/blog_Regression_project.html#what-i-did",
    "title": "Predicting Medical Costs - A Regression Problem",
    "section": "4. What I Did",
    "text": "4. What I Did\nExplored the medical cost dataset with visuals — distributions, correlations, and feature effects — to link age, BMI, and smoking to charges. Cleaned numeric features with StandardScaler and categorical variables with OneHotEncoder. Trained LinearRegression, SVR, DecisionTreeRegressor, and RandomForestRegressor, tuning hyperparameters with RandomizedSearchCV. Evaluated MAE, MSE, RMSE, and R² on the test set and compared models visually."
  },
  {
    "objectID": "posts/02_Medical_Costs_Regression/blog_Regression_project.html#what-i-found",
    "href": "posts/02_Medical_Costs_Regression/blog_Regression_project.html#what-i-found",
    "title": "Predicting Medical Costs - A Regression Problem",
    "section": "5. What I Found",
    "text": "5. What I Found\nInsurance charges tend to increase with age. Smokers have significantly higher and more variable charges compared to non-smokers, whose costs show a smoother, more consistent trend.\nAmong smokers, insurance charges generally increase with higher BMI, while BMI has little impact on costs for non-smokers. Within non-smokers, age drives costs.\nFeature importance analysis (from the tree-based models) highlighted smoking, age, and BMI as the top predictors — confirming both domain expectations and model consistency.\nThe Random Forest Regressor achieved the best results, with the lowest RMSE (≈ 4354) and highest R² (≈ 0.88), indicating strong predictive accuracy and generalization on unseen data, showing that it is possible to predict medical costs based on key personal and lifestyle factors. This demonstrates that data-driven models can effectively capture the underlying relationships between health behaviors and medical expenses, enabling more accurate cost forecasting and better-informed healthcare and policy decisions.\n\n\n\nFig. 3 Charges increase with age, and smokers have significantly higher and more variable charges than non-smokers.\n\n\n\n\n\nFig. 4 Smokers consistently have higher charges than non-smokers. Among smokers, charges increase strongly with BMI (r ≈ 0.81), while among non-smokers, BMI has almost no relationship with charges (r ≈ 0.08). This indicates a strong interaction effect — smoking amplifies the impact of BMI on charges.\n\n\n\n\n\nFig. 5 Decision tree regression model predicting medical insurance charges. The model splits data points into groups based on key factors. The most important splitting criterion is smoking status. Among non-smokers, age is the most influential variable, while for smokers, BMI plays the dominant role. The “value” shown in each node represents the model’s predicted target value (here, the estimated insurance cost in dollars).\n\n\n\n\n\nFig. 6 Comparison of Root Mean Squared Error (RMSE) values for different regression models on the test set. The RMSE measures the average magnitude of prediction errors, with lower values indicating better model performance. The Random Forest Regressor achieved the lowest RMSE, followed by the Decision Tree Regressor, while the SVR and Linear Regression models showed higher error levels, suggesting less accurate predictions.)\n\n\n\n\n\nFig. 7 Comparison of R² scores for different regression models on the test set. The R² score represents the proportion of variance in medical insurance charges explained by each model. Higher values indicate better predictive performance. The Random Forest Regressor achieved the highest R², followed closely by the Decision Tree Regressor, while Linear Regression and SVR showed comparatively lower explanatory power."
  },
  {
    "objectID": "posts/02_Medical_Costs_Regression/blog_Regression_project.html#what-i-learned",
    "href": "posts/02_Medical_Costs_Regression/blog_Regression_project.html#what-i-learned",
    "title": "Predicting Medical Costs - A Regression Problem",
    "section": "6. What I Learned",
    "text": "6. What I Learned\nModel performance improved with complexity:\nTree-based models, especially Random Forest, captured the nonlinear impact of health and lifestyle factors most effectively. The findings align with real-world intuition — smokers and older individuals face substantially higher insurance costs."
  },
  {
    "objectID": "posts/02_Medical_Costs_Regression/blog_Regression_project.html#whats-next",
    "href": "posts/02_Medical_Costs_Regression/blog_Regression_project.html#whats-next",
    "title": "Predicting Medical Costs - A Regression Problem",
    "section": "7. What’s Next",
    "text": "7. What’s Next\nThis project shows that predicting medical costs is possible — but there’s room to grow. Future work could explore more advanced models like Gradient Boosting or XGBoost to see if they capture even subtler patterns in the data. Another direction is adding richer features to improve model depth and realism. Finally, scaling this work to larger, more diverse datasets would help assess how well these findings hold up across different populations. The next steps are open — it’s up to you to take this foundation and push it further."
  },
  {
    "objectID": "posts/02_Medical_Costs_Regression/blog_Regression_project.html#eda",
    "href": "posts/02_Medical_Costs_Regression/blog_Regression_project.html#eda",
    "title": "Predicting Medical Costs - A Regression Problem",
    "section": "EDA",
    "text": "EDA\nChecking the structure of the DataFrame\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1338 entries, 0 to 1337\nData columns (total 7 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   age       1338 non-null   int64  \n 1   sex       1338 non-null   object \n 2   bmi       1338 non-null   float64\n 3   children  1338 non-null   int64  \n 4   smoker    1338 non-null   object \n 5   region    1338 non-null   object \n 6   charges   1338 non-null   float64\ndtypes: float64(2), int64(2), object(3)\nmemory usage: 73.3+ KB\n\n\nChecking for missing values\n\n# -&gt; no missing values\ndf.isnull().sum()\n\nage         0\nsex         0\nbmi         0\nchildren    0\nsmoker      0\nregion      0\ncharges     0\ndtype: int64\n\n\nCreating a new column with catrgorical BMI features\nBMI Categories\n\n\n\nBMI Category\nBMI Range\n\n\n\n\nUnderweight\nBelow 18.5\n\n\nHealthy\n18.5 – 24.9\n\n\nOverweight\n25.0 – 29.9\n\n\nObesity\n30.0 or above\n\n\n\n\nbins = [0, 18.5, 25, 30, float(\"inf\")]\nlabels = [\"underweight\", \"normal\", \"overweight\", \"obese\"]\n\ndf[\"BMI_category\"] = pd.cut(df[\"bmi\"], bins=bins, labels=labels, right=False)\n\nPlotting the distribution of the numeric columns\n\nnumeric_columns = ['age', 'bmi', 'children', 'charges']\n\nfig, axes = plt.subplots(2, 2, figsize=(12,10), sharey=False)\nfig.suptitle(\"Distribution of Numeric Features\", fontsize=18, weight=\"bold\")\n\nR, C = axes.shape\n\nfor k, (ax, col) in enumerate(zip(axes.flat, numeric_columns)):\n\n    j = k % C # 0 % 2 = 0; 1%2 = 1\n\n    sns.histplot(\n        data=df,\n        x=col,\n        bins=40,\n        ax=ax,\n        stat=\"density\",\n        color=\"#69b3a2\",\n        edgecolor=\"white\",\n        alpha=0.85,\n    )\n    sns.kdeplot(\n        data=df,\n        x=col,\n        ax=ax,\n        color=\"#b22222\",\n        linewidth=2.5,\n    )\n\n    #ax.set_title(col.capitalize(), fontsize=14, weight=\"semibold\")\n    ax.set_xlabel(col.capitalize(), weight=\"semibold\")\n    ax.set_ylabel(\"Density\" if j == 0 else \"\")\n    ax.set_xlim(left=0)\n    ax.tick_params(axis=\"both\", labelsize=12)\n\n    ax.set_facecolor(\"#f7f7f7\")\n\nfig.tight_layout(pad=1.5)\n\nsave_fig(\"numeric_dist_histplot\")\n\n\n\n\n\n\n\n\nPlotting the distribution of categorical columns\n\ncategorial_columns = ['sex', 'smoker', 'region', 'BMI_category']\n\nfig, axes = plt.subplots(2, 2, figsize=(10,8), sharey=False)\nfig.suptitle(\"Distribution of Categorical Features\", fontsize=18, weight=\"bold\")\n\nR, C = axes.shape\n\nfor k, (ax, col) in enumerate(zip(axes.flat, categorial_columns)):\n\n    j = k % C\n\n    sns.histplot(\n        data=df,\n        x=col,\n        bins=40,\n        ax=ax,\n        stat=\"count\",\n        color=\"#69b3a2\",\n        edgecolor=\"white\",\n        alpha=0.85,\n    )\n\n    ax.set_title(col.capitalize(), fontsize=14, weight=\"semibold\")\n    ax.set_xlabel(\"\")\n    ax.set_ylabel(\"Count\" if j == 0 else \"\")\n    ax.tick_params(axis=\"both\", labelsize=12)\n\n    ax.tick_params(axis=\"both\", labelsize=12)\n\n    ax.set_facecolor(\"#f7f7f7\")\n\nfig.tight_layout(pad=1.5)\n\nsave_fig(\"cat_dist_histplot\")\n\n\n\n\n\n\n\n\n\ndf[\"sex\"].value_counts()\n\nsex\nmale      676\nfemale    662\nName: count, dtype: int64\n\n\n\ndf[\"smoker\"].value_counts()\n\nsmoker\nno     1064\nyes     274\nName: count, dtype: int64\n\n\n\ndf[\"region\"].value_counts()\n\nregion\nsoutheast    364\nsouthwest    325\nnorthwest    325\nnortheast    324\nName: count, dtype: int64\n\n\n\ndf[\"BMI_category\"].value_counts()\n\nBMI_category\nobese          707\noverweight     386\nnormal         225\nunderweight     20\nName: count, dtype: int64\n\n\nTaking a look at the target variable\n\ndf[\"charges\"].describe()\n\ncount     1338.000000\nmean     13270.422265\nstd      12110.011237\nmin       1121.873900\n25%       4740.287150\n50%       9382.033000\n75%      16639.912515\nmax      63770.428010\nName: charges, dtype: float64\n\n\n\n# Insights:\n# Charges generally rise with age\n# Smokers have higher charges than non-smokers\n# Smokers not only have higher average charges, but their costs are much more spread out\n# non-smokers trend is smoother\n\nsns.scatterplot(df, x=\"age\", y=\"charges\", alpha=0.5, hue=\"smoker\")\nplt.title(\"Charges by Age\", fontsize=18, weight=\"bold\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Charges\")\nplt.legend(loc=\"best\", title=\"smoker\")\n\nax = plt.gca()\nax.set_facecolor(\"#f7f7f7\")\nplt.tight_layout()\n\nsave_fig(\"age_vs_charges\")\n\n\n\n\n\n\n\n\n\n# Insights\n# Charges increase with age across all BMI categories.\n# However, BMI category alone does not create clear separation in charges — there’s considerable overlap between normal, overweight, and obese groups.\n# The large vertical gaps between charge levels likely stem from other variables (e.g., smoking).\n# Overall, BMI has a smaller impact on charges than age or smoking status.\n\n\nplt.figure(figsize=(9,6))\nsns.scatterplot(df, x=\"age\", y=\"charges\", alpha=0.5, hue=\"BMI_category\")\nplt.title(\"Charges by Age\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Charges\")\nplt.legend(loc=[1.05, 0.3], title=\"BMI Category\")\n\nax = plt.gca()\nax.set_facecolor(\"#f7f7f7\")\n\nplt.tight_layout()\n\nsave_fig(\"age_vs_charges_bmi\")\n\n\n\n\n\n\n\n\n\n# Insights:\n# For somkers Charges generally rise with BMI\n# Smokers have higher charges than non-smokers\n# For non-smokers bmi has little effect\n\nsns.scatterplot(df, x=\"bmi\", y=\"charges\", alpha=0.5, hue=\"smoker\") # higher BMI -&gt; higher charges\nplt.title(\"Charges by BMI\", fontsize=18, weight=\"bold\")\nplt.xlabel(\"BMI\")\nplt.ylabel(\"Charges\")\n\nplt.legend(loc=\"best\", title=\"smoker\")\n\nax = plt.gca()\nax.set_facecolor(\"#f7f7f7\")\nplt.tight_layout()\n\nsave_fig(\"bmi_vs_charges\")\n\n\n\n\n\n\n\n\nCheck the correlation between Smoking and BMI on the charges\n\ndf[df.smoker==\"yes\"].charges.corr(df[df.smoker==\"yes\"].bmi)\n\n0.8064806070155408\n\n\n\ndf[df.smoker==\"no\"].charges.corr(df[df.smoker==\"no\"].bmi)\n\n0.08403654312833271\n\n\n\n# Insights:\n# Charges strongly correlate to being a smoker\n# Age and BMI follow respectively\n\ndf[\"smoker_flag\"] = df[\"smoker\"].map({\"yes\": 1, \"no\": 0})\ndf[\"sex_flag\"] = df[\"sex\"].map({\"female\": 0, \"male\": 1})\ndf.corr(numeric_only=True)[\"charges\"].sort_values(ascending=False)\n\ncharges        1.000000\nsmoker_flag    0.787251\nage            0.299008\nbmi            0.198341\nchildren       0.067998\nsex_flag       0.057292\nName: charges, dtype: float64"
  },
  {
    "objectID": "posts/02_Medical_Costs_Regression/blog_Regression_project.html#preprocessing",
    "href": "posts/02_Medical_Costs_Regression/blog_Regression_project.html#preprocessing",
    "title": "Predicting Medical Costs - A Regression Problem",
    "section": "Preprocessing",
    "text": "Preprocessing\n\ndf = pd.read_csv(\"data/insurance.csv\")\n\n\ndf.head()\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n\n\n\n\n\n\n\n\nnum_pipeline = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", StandardScaler())\n])\n\ncat_pipeline = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"onehot\", OneHotEncoder(drop=\"if_binary\"))\n])\n\n\nfrom sklearn.compose import ColumnTransformer, make_column_selector\n\n\npreprocessing = ColumnTransformer([\n    (\"num\", num_pipeline, make_column_selector(dtype_include=np.number)),\n    (\"cat\", cat_pipeline, make_column_selector(dtype_include=object))\n])\n\n\ntree_num_pipeline = Pipeline([\n   (\"imputer\", SimpleImputer(strategy=\"median\")) \n])\n\ntree_preprocessing = ColumnTransformer([\n    (\"num\", tree_num_pipeline, make_column_selector(dtype_include=np.number)),\n    (\"cat\", cat_pipeline, make_column_selector(dtype_include=object))\n])\n\n\nTrain Test Split\n\nX = df.drop(\"charges\", axis=1)\n\n\ny = df[\"charges\"]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n\n\nprint(f\"X_train: {X_train.shape}; y_train: {y_train.shape}\\nX_test: {X_test.shape}; y_test: {y_test.shape}\")\n\nX_train: (1137, 6); y_train: (1137,)\nX_test: (201, 6); y_test: (201,)"
  },
  {
    "objectID": "posts/02_Medical_Costs_Regression/blog_Regression_project.html#training-a-model",
    "href": "posts/02_Medical_Costs_Regression/blog_Regression_project.html#training-a-model",
    "title": "Predicting Medical Costs - A Regression Problem",
    "section": "Training a Model",
    "text": "Training a Model\n\ndef metrics(y_true, y_pred):\n    MAE = mean_absolute_error(y_true, y_pred)\n    print(f\"MAE: {MAE}\")\n\n    MSE = mean_squared_error(y_true, y_pred)\n    RMSE = np.sqrt(MSE)\n    print(f\"MSE: {MSE}\\nRMSE: {RMSE}\")\n\n\nLinearRegression\n\nlinear_reg = make_pipeline(preprocessing, LinearRegression())\n\n\nlinear_reg_score = -cross_val_score(linear_reg, X_train, y_train, cv=5, scoring=\"neg_root_mean_squared_error\")\n\n\npd.Series(linear_reg_score).describe()\n\ncount       5.000000\nmean     6173.951563\nstd       211.241338\nmin      5926.581771\n25%      6070.880265\n50%      6156.104487\n75%      6219.280117\nmax      6496.911172\ndtype: float64\n\n\n\n\nSVR\n\nsvr_model = make_pipeline(preprocessing, SVR())\n\n\nsvr_score = -cross_val_score(svr_model, X_train, y_train, cv=5, scoring=\"neg_root_mean_squared_error\")\n\n\npd.Series(svr_score).describe()\n\ncount        5.000000\nmean     12595.037950\nstd       1099.948405\nmin      11392.920185\n25%      11770.593709\n50%      12616.341217\n75%      13004.470299\nmax      14190.864341\ndtype: float64\n\n\n\nparam_grid = {\n    \"svr__C\": [0.1, 1, 10, 1000],\n    \"svr__gamma\": ['scale', 'auto', 0.01, 0.1, 1],\n    \"svr__kernel\": ['rbf', 'linear'],\n    \"svr__epsilon\": [0.01, 0.1, 0.5]\n}\n\nsvr_tuned = RandomizedSearchCV(svr_model, param_distributions=param_grid, cv=3, n_iter= 100, scoring=\"neg_root_mean_squared_error\", random_state=42, n_jobs=-1, verbose=3)\n\nsvr_tuned.fit(X_train, y_train)\n\nFitting 3 folds for each of 100 candidates, totalling 300 fits\n\n\nRandomizedSearchCV(cv=3,\n                   estimator=Pipeline(steps=[('columntransformer',\n                                              ColumnTransformer(transformers=[('num',\n                                                                               Pipeline(steps=[('imputer',\n                                                                                                SimpleImputer(strategy='median')),\n                                                                                               ('scaler',\n                                                                                                StandardScaler())]),\n                                                                               &lt;sklearn.compose._column_transformer.make_column_selector object at 0x00000269773A09B0&gt;),\n                                                                              ('cat',\n                                                                               Pipeline(steps=[('imputer',\n                                                                                                SimpleImputer(strategy='most_...\n                                                                                                OneHotEncoder(drop='if_binary'))]),\n                                                                               &lt;sklearn.compose._column_transformer.make_column_selector object at 0x00000269773A2780&gt;)])),\n                                             ('svr', SVR())]),\n                   n_iter=100, n_jobs=-1,\n                   param_distributions={'svr__C': [0.1, 1, 10, 1000],\n                                        'svr__epsilon': [0.01, 0.1, 0.5],\n                                        'svr__gamma': ['scale', 'auto', 0.01,\n                                                       0.1, 1],\n                                        'svr__kernel': ['rbf', 'linear']},\n                   random_state=42, scoring='neg_root_mean_squared_error',\n                   verbose=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=3,\n                   estimator=Pipeline(steps=[('columntransformer',\n                                              ColumnTransformer(transformers=[('num',\n                                                                               Pipeline(steps=[('imputer',\n                                                                                                SimpleImputer(strategy='median')),\n                                                                                               ('scaler',\n                                                                                                StandardScaler())]),\n                                                                               &lt;sklearn.compose._column_transformer.make_column_selector object at 0x00000269773A09B0&gt;),\n                                                                              ('cat',\n                                                                               Pipeline(steps=[('imputer',\n                                                                                                SimpleImputer(strategy='most_...\n                                                                                                OneHotEncoder(drop='if_binary'))]),\n                                                                               &lt;sklearn.compose._column_transformer.make_column_selector object at 0x00000269773A2780&gt;)])),\n                                             ('svr', SVR())]),\n                   n_iter=100, n_jobs=-1,\n                   param_distributions={'svr__C': [0.1, 1, 10, 1000],\n                                        'svr__epsilon': [0.01, 0.1, 0.5],\n                                        'svr__gamma': ['scale', 'auto', 0.01,\n                                                       0.1, 1],\n                                        'svr__kernel': ['rbf', 'linear']},\n                   random_state=42, scoring='neg_root_mean_squared_error',\n                   verbose=3)estimator: PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x00000269773A09B0&gt;),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(drop='if_binary'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x00000269773A2780&gt;)])),\n                ('svr', SVR())])columntransformer: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x00000269773A09B0&gt;),\n                                ('cat',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('onehot',\n                                                  OneHotEncoder(drop='if_binary'))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x00000269773A2780&gt;)])num&lt;sklearn.compose._column_transformer.make_column_selector object at 0x00000269773A09B0&gt;SimpleImputerSimpleImputer(strategy='median')StandardScalerStandardScaler()cat&lt;sklearn.compose._column_transformer.make_column_selector object at 0x00000269773A2780&gt;SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary')SVRSVR()\n\n\n\nsvr_score = -svr_tuned.best_score_\nsvr_score\n\n7333.541404442819\n\n\n\nsvr_tuned.best_params_\n\n{'svr__kernel': 'linear',\n 'svr__gamma': 'scale',\n 'svr__epsilon': 0.01,\n 'svr__C': 1000}\n\n\n\n\nDecisionTree\nUsing a simple DecisionTree as a baseline\n\ndt = make_pipeline(tree_preprocessing, DecisionTreeRegressor(max_depth=3))\ndt.fit(X_train, y_train)\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x00000269760A4AD0&gt;),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(drop='if_binary'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000002696F9D7DA0&gt;)])),\n                ('decisiontreeregressor', DecisionTreeRegressor(max_depth=3))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x00000269760A4AD0&gt;),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(drop='if_binary'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000002696F9D7DA0&gt;)])),\n                ('decisiontreeregressor', DecisionTreeRegressor(max_depth=3))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='median'))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x00000269760A4AD0&gt;),\n                                ('cat',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('onehot',\n                                                  OneHotEncoder(drop='if_binary'))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000002696F9D7DA0&gt;)])num&lt;sklearn.compose._column_transformer.make_column_selector object at 0x00000269760A4AD0&gt;SimpleImputerSimpleImputer(strategy='median')cat&lt;sklearn.compose._column_transformer.make_column_selector object at 0x000002696F9D7DA0&gt;SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary')DecisionTreeRegressorDecisionTreeRegressor(max_depth=3)\n\n\n\nbase_decision_tree_score = -cross_val_score(dt, X_train, y_train, cv=5, scoring=\"neg_root_mean_squared_error\")\n\n\npd.Series(base_decision_tree_score).describe()\n\ncount       5.000000\nmean     4869.172374\nstd       295.376724\nmin      4446.480626\n25%      4768.196718\n50%      4829.922649\n75%      5142.062041\nmax      5159.199837\ndtype: float64\n\n\nUsing RandomizedSearch to find the best parameters for the Decisiontree\n\nparam_grid = {\n    \"decisiontreeregressor__max_depth\": range(2,20),\n    \"decisiontreeregressor__min_samples_split\": range(2,10),\n    \"decisiontreeregressor__min_samples_leaf\": range(1,10),\n    \"decisiontreeregressor__max_features\": [\"sqrt\", \"log2\", None]\n}\n\nrnd_decision_tree = RandomizedSearchCV(dt, param_distributions=param_grid, n_iter=100, cv=3, scoring=\"neg_root_mean_squared_error\", random_state=42)\n\nrnd_decision_tree.fit(X_train, y_train)\n\nRandomizedSearchCV(cv=3,\n                   estimator=Pipeline(steps=[('columntransformer',\n                                              ColumnTransformer(transformers=[('num',\n                                                                               Pipeline(steps=[('imputer',\n                                                                                                SimpleImputer(strategy='median'))]),\n                                                                               &lt;sklearn.compose._column_transformer.make_column_selector object at 0x00000269760A4AD0&gt;),\n                                                                              ('cat',\n                                                                               Pipeline(steps=[('imputer',\n                                                                                                SimpleImputer(strategy='most_frequent')),\n                                                                                               ('onehot',\n                                                                                                OneHot...\n                                             ('decisiontreeregressor',\n                                              DecisionTreeRegressor(max_depth=3))]),\n                   n_iter=100,\n                   param_distributions={'decisiontreeregressor__max_depth': range(2, 20),\n                                        'decisiontreeregressor__max_features': ['sqrt',\n                                                                                'log2',\n                                                                                None],\n                                        'decisiontreeregressor__min_samples_leaf': range(1, 10),\n                                        'decisiontreeregressor__min_samples_split': range(2, 10)},\n                   random_state=42, scoring='neg_root_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=3,\n                   estimator=Pipeline(steps=[('columntransformer',\n                                              ColumnTransformer(transformers=[('num',\n                                                                               Pipeline(steps=[('imputer',\n                                                                                                SimpleImputer(strategy='median'))]),\n                                                                               &lt;sklearn.compose._column_transformer.make_column_selector object at 0x00000269760A4AD0&gt;),\n                                                                              ('cat',\n                                                                               Pipeline(steps=[('imputer',\n                                                                                                SimpleImputer(strategy='most_frequent')),\n                                                                                               ('onehot',\n                                                                                                OneHot...\n                                             ('decisiontreeregressor',\n                                              DecisionTreeRegressor(max_depth=3))]),\n                   n_iter=100,\n                   param_distributions={'decisiontreeregressor__max_depth': range(2, 20),\n                                        'decisiontreeregressor__max_features': ['sqrt',\n                                                                                'log2',\n                                                                                None],\n                                        'decisiontreeregressor__min_samples_leaf': range(1, 10),\n                                        'decisiontreeregressor__min_samples_split': range(2, 10)},\n                   random_state=42, scoring='neg_root_mean_squared_error')estimator: PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x00000269760A4AD0&gt;),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(drop='if_binary'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000002696F9D7DA0&gt;)])),\n                ('decisiontreeregressor', DecisionTreeRegressor(max_depth=3))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='median'))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x00000269760A4AD0&gt;),\n                                ('cat',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('onehot',\n                                                  OneHotEncoder(drop='if_binary'))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000002696F9D7DA0&gt;)])num&lt;sklearn.compose._column_transformer.make_column_selector object at 0x00000269760A4AD0&gt;SimpleImputerSimpleImputer(strategy='median')cat&lt;sklearn.compose._column_transformer.make_column_selector object at 0x000002696F9D7DA0&gt;SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary')DecisionTreeRegressorDecisionTreeRegressor(max_depth=3)\n\n\n\nrnd_decision_tree.best_params_\n\n{'decisiontreeregressor__min_samples_split': 4,\n 'decisiontreeregressor__min_samples_leaf': 9,\n 'decisiontreeregressor__max_features': None,\n 'decisiontreeregressor__max_depth': 4}\n\n\n\n-rnd_decision_tree.best_score_\n\n4770.267010134789\n\n\n\nrnd_decision_tree.best_estimator_\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000002694062E750&gt;),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(drop='if_binary'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x0000026976C13410&gt;)])),\n                ('decisiontreeregressor',\n                 DecisionTreeRegressor(max_depth=4, min_samples_leaf=9,\n                                       min_samples_split=4))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000002694062E750&gt;),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(drop='if_binary'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x0000026976C13410&gt;)])),\n                ('decisiontreeregressor',\n                 DecisionTreeRegressor(max_depth=4, min_samples_leaf=9,\n                                       min_samples_split=4))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='median'))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000002694062E750&gt;),\n                                ('cat',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('onehot',\n                                                  OneHotEncoder(drop='if_binary'))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x0000026976C13410&gt;)])num&lt;sklearn.compose._column_transformer.make_column_selector object at 0x000002694062E750&gt;SimpleImputerSimpleImputer(strategy='median')cat&lt;sklearn.compose._column_transformer.make_column_selector object at 0x0000026976C13410&gt;SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary')DecisionTreeRegressorDecisionTreeRegressor(max_depth=4, min_samples_leaf=9, min_samples_split=4)\n\n\n\ndecision_tree_score = -cross_val_score(rnd_decision_tree.best_estimator_, X_train, y_train, cv=5, scoring=\"neg_root_mean_squared_error\")\n\n\npd.Series(decision_tree_score).describe()\n\ncount       5.000000\nmean     4752.431458\nstd       311.946236\nmin      4308.317556\n25%      4619.560739\n50%      4755.759409\n75%      4969.267050\nmax      5109.252536\ndtype: float64\n\n\n\nbest_tree = rnd_decision_tree.best_estimator_[\"decisiontreeregressor\"]\nbest_tree\n\nDecisionTreeRegressor(max_depth=4, min_samples_leaf=9, min_samples_split=4)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=4, min_samples_leaf=9, min_samples_split=4)\n\n\n\n# Insights:\n# First splits on smoking -&gt;  confirming smoking dominates the cost signal\n# For the smokers, BMI is the next important factor\n# For non-smokers age is the next important factor\n\n\nfeature_names = rnd_decision_tree.best_estimator_[\"columntransformer\"].get_feature_names_out()\npretty_names = [f.split(\"__\")[-1] for f in feature_names]\n\n\nfig, ax = plt.subplots(figsize=(11, 8), dpi=400)\n\nplot_tree(\n     best_tree,\n     max_depth=3,\n     feature_names=pretty_names,\n     filled=True,\n     rounded=True,\n     fontsize=10,\n     proportion=True, # show % of samples instead of counts\n     impurity=False, # hide squared_error\n     precision=1, # shorter numbers\n     ax=ax\n     \n)\n\nplt.title(\"Decision Tree\", fontsize=18, weight=\"bold\")\nplt.tight_layout()\n\nsave_fig(\"Decision_Tree_plot\", resolution=400)\n\n\n\n\n\n\n\n\nSmaller iamges for the Blog:\n\nfeature_names = rnd_decision_tree.best_estimator_[\"columntransformer\"].get_feature_names_out()\npretty_names = [f.split(\"__\")[-1] for f in feature_names]\n\n\nfig, ax = plt.subplots(figsize=(8, 6), dpi=300)\n\nplot_tree(\n     best_tree,\n     max_depth=2,\n     feature_names=pretty_names,\n     filled=True,\n     rounded=True,\n     fontsize=10,\n     proportion=True, # show % of samples instead of counts\n     impurity=False, # hide squared_error\n     precision=1, # shorter numbers\n     ax=ax\n     \n)\n\nplt.title(\"Decision Tree\", fontsize=18, weight=\"bold\")\nplt.tight_layout()\n\nsave_fig(\"Decision_Tree_plot_small\")\n\n\n\n\n\n\n\n\n\n\nRandomForestRegressor\n\nrfr = make_pipeline(tree_preprocessing, RandomForestRegressor())\n\n\nbase_random_forest_score = -cross_val_score(rfr, X_train, y_train, cv=5, scoring=\"neg_root_mean_squared_error\")\n\n\npd.Series(base_random_forest_score).describe()\n\ncount       5.000000\nmean     4877.139833\nstd       293.198791\nmin      4441.898586\n25%      4733.486913\n50%      4949.047954\n75%      5123.462899\nmax      5137.802811\ndtype: float64\n\n\n\n\nparam_grid = {\n    \"randomforestregressor__n_estimators\": range(50,200),\n    \"randomforestregressor__max_depth\": range(2,40),\n    \"randomforestregressor__min_samples_split\": range(2,10),\n    \"randomforestregressor__min_samples_leaf\": range(1,10),\n    \"randomforestregressor__max_features\": [\"sqrt\", \"log2\", None]\n}\n\nrnd_forest = RandomizedSearchCV(rfr, param_distributions=param_grid, n_iter=100, scoring=\"neg_root_mean_squared_error\", random_state=42)\n\nrnd_forest.fit(X_train, y_train)\n\nRandomizedSearchCV(estimator=Pipeline(steps=[('columntransformer',\n                                              ColumnTransformer(transformers=[('num',\n                                                                               Pipeline(steps=[('imputer',\n                                                                                                SimpleImputer(strategy='median'))]),\n                                                                               &lt;sklearn.compose._column_transformer.make_column_selector object at 0x00000269760A4AD0&gt;),\n                                                                              ('cat',\n                                                                               Pipeline(steps=[('imputer',\n                                                                                                SimpleImputer(strategy='most_frequent')),\n                                                                                               ('onehot',\n                                                                                                OneHotEncod...\n                   n_iter=100,\n                   param_distributions={'randomforestregressor__max_depth': range(2, 40),\n                                        'randomforestregressor__max_features': ['sqrt',\n                                                                                'log2',\n                                                                                None],\n                                        'randomforestregressor__min_samples_leaf': range(1, 10),\n                                        'randomforestregressor__min_samples_split': range(2, 10),\n                                        'randomforestregressor__n_estimators': range(50, 200)},\n                   random_state=42, scoring='neg_root_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(estimator=Pipeline(steps=[('columntransformer',\n                                              ColumnTransformer(transformers=[('num',\n                                                                               Pipeline(steps=[('imputer',\n                                                                                                SimpleImputer(strategy='median'))]),\n                                                                               &lt;sklearn.compose._column_transformer.make_column_selector object at 0x00000269760A4AD0&gt;),\n                                                                              ('cat',\n                                                                               Pipeline(steps=[('imputer',\n                                                                                                SimpleImputer(strategy='most_frequent')),\n                                                                                               ('onehot',\n                                                                                                OneHotEncod...\n                   n_iter=100,\n                   param_distributions={'randomforestregressor__max_depth': range(2, 40),\n                                        'randomforestregressor__max_features': ['sqrt',\n                                                                                'log2',\n                                                                                None],\n                                        'randomforestregressor__min_samples_leaf': range(1, 10),\n                                        'randomforestregressor__min_samples_split': range(2, 10),\n                                        'randomforestregressor__n_estimators': range(50, 200)},\n                   random_state=42, scoring='neg_root_mean_squared_error')estimator: PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x00000269760A4AD0&gt;),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(drop='if_binary'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000002696F9D7DA0&gt;)])),\n                ('randomforestregressor', RandomForestRegressor())])columntransformer: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='median'))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x00000269760A4AD0&gt;),\n                                ('cat',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('onehot',\n                                                  OneHotEncoder(drop='if_binary'))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000002696F9D7DA0&gt;)])num&lt;sklearn.compose._column_transformer.make_column_selector object at 0x00000269760A4AD0&gt;SimpleImputerSimpleImputer(strategy='median')cat&lt;sklearn.compose._column_transformer.make_column_selector object at 0x000002696F9D7DA0&gt;SimpleImputerSimpleImputer(strategy='most_frequent')OneHotEncoderOneHotEncoder(drop='if_binary')RandomForestRegressorRandomForestRegressor()\n\n\n\nrnd_forest.best_params_\n\n{'randomforestregressor__n_estimators': 58,\n 'randomforestregressor__min_samples_split': 7,\n 'randomforestregressor__min_samples_leaf': 3,\n 'randomforestregressor__max_features': None,\n 'randomforestregressor__max_depth': 5}\n\n\n\n-rnd_forest.best_score_\n\n4621.142167648941\n\n\n\nrandom_forest_score = -cross_val_score(rnd_forest.best_estimator_, X_train, y_train, cv=5, scoring=\"neg_root_mean_squared_error\")\n\n\npd.Series(random_forest_score).describe()\n\ncount       5.000000\nmean     4616.455082\nstd       359.807740\nmin      4048.132168\n25%      4533.837089\n50%      4709.176946\n75%      4786.871966\nmax      5004.257239\ndtype: float64\n\n\n\n\nComparing Models\n\n# Training all models on the full training set and then predicting on the test set\n\nmodels = [linear_reg, svr_tuned.best_estimator_, rnd_decision_tree.best_estimator_, rnd_forest.best_estimator_]\nmodel_names = [\"LinearRegression\", \"SVR\", \"DecisionTreeRegressor\", \"RandomForestRegressor\"]\n\nall_scores = {}\n\nfor name, model in zip(model_names, models):\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n\n    MAE = mean_absolute_error(y_test, y_pred)\n    MSE = mean_squared_error(y_test, y_pred)\n    RMSE = np.sqrt(MSE)\n    r2 = r2_score(y_test, y_pred)\n\n    all_scores[name] = {\"MAE\": round(MAE,2), \"MSE\": round(MSE,2), \"RMSE\": round(RMSE,2), \"R2\": round(r2,2)}\n\ndf_model_comparison = pd.DataFrame(all_scores).T\n\n\ndf_model_comparison.index.name = \"Models\"\n\n\ndf_model_comparison\n\n\n\n\n\n\n\n\nMAE\nMSE\nRMSE\nR2\n\n\nModels\n\n\n\n\n\n\n\n\nLinearRegression\n4092.36\n31333268.50\n5597.61\n0.79\n\n\nSVR\n3373.30\n37706594.68\n6140.57\n0.75\n\n\nDecisionTreeRegressor\n2755.59\n21655894.86\n4653.59\n0.86\n\n\nRandomForestRegressor\n2459.69\n18958974.55\n4354.19\n0.88\n\n\n\n\n\n\n\n\ndf_model_comparison.to_csv(\"data/model_comparison_metrics.csv\")\n\n\n# comparing RMSE across all models\n\nplt.figure(figsize=(10,7))\nsns.barplot(data=df_model_comparison, x=\"Models\", y=\"RMSE\")\nplt.title(\"Model Comparison: RMSE\", fontsize=18, weight=\"bold\")\nplt.xlabel(\"Model\")\nplt.ylabel(\"RMSE\")\n#plt.ylim(0,1)\n\nplt.tight_layout()\n\nsave_fig(\"RMSE_score\")\n\n\n\n\n\n\n\n\n\n# Normalize so that lowest RMSE → 1, highest → 0\nrmse_norm = 1 - (df_model_comparison[\"RMSE\"] - df_model_comparison[\"RMSE\"].min()) / (df_model_comparison[\"RMSE\"].max() - df_model_comparison[\"RMSE\"].min())\n\n\nrmse_norm\n\nModels\nLinearRegression         0.303944\nSVR                      0.000000\nDecisionTreeRegressor    0.832398\nRandomForestRegressor    1.000000\nName: RMSE, dtype: float64\n\n\n\n# comparing RMSE across all models\n\nplt.figure(figsize=(10,7))\nsns.barplot(data=df_model_comparison, x=\"Models\", y=rmse_norm)\nplt.title(\"Model Comparison: Normalized RMSE\", fontsize=18, weight=\"bold\")\nplt.xlabel(\"Model\")\nplt.ylabel(\"Normalized RMSE Score\")\n#plt.ylim(0,1)\n\nplt.tight_layout()\n\nsave_fig(\"RMSE_score_norm\")\n\n\n\n\n\n\n\n\n\n# comparing R2 score across all models\n\nplt.figure(figsize=(10,7))\nsns.barplot(data=df_model_comparison, x=\"Models\", y=\"R2\")\nplt.title(\"Model Comparison: R² Score\", fontsize=18, weight=\"bold\")\nplt.xlabel(\"Model\")\nplt.ylabel(\"R2\")\nplt.ylim(0,1)\n\nplt.tight_layout()\n\nsave_fig(\"R2_score\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Topics",
    "section": "",
    "text": "Hello Blog!\n\n\n\n\n\n\n\n\nOct 25, 2025\n\n\nPatrick Linke\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\nPredicting Medical Costs - A Regression Problem\n\n\n\nRegression\n\nMedicine\n\n\n\n\n\n\n\n\n\nOct 26, 2025\n\n\nPatrick Linke\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "contact.html#lets-connect",
    "href": "contact.html#lets-connect",
    "title": "Contact",
    "section": "Let’s Connect",
    "text": "Let’s Connect\nI’d love to connect with fellow machine learning enthusiasts, especially those interested in healthcare applications and data science.\n\nLinkedIn: My LinkedIn Profile\nGitHub: My GitHub Profile\n\n\nAlways happy to chat about ML, medicine, and the intersection of both!"
  },
  {
    "objectID": "about.html#who-am-i",
    "href": "about.html#who-am-i",
    "title": "About Me",
    "section": "Who am I?",
    "text": "Who am I?\nHi, I’m Patrick — a medical student fascinated by how machine learning and artificial intelligence are reshaping healthcare.\nThis blog is where I document my journey — learning, experimenting, and sharing insights as I explore how data and algorithms can improve decision-making."
  },
  {
    "objectID": "about.html#why-this-blog-exists",
    "href": "about.html#why-this-blog-exists",
    "title": "About Me",
    "section": "Why This Blog Exists",
    "text": "Why This Blog Exists\nThis blog represents my commitment to learning in public—documenting my journey as I build hands-on projects and deepen my understanding of machine learning. Here’s what drives me:\n\nBridging domains: Exploring how machine learning can enhance medical research and clinical practice\nBuilding intuition: Working through real datasets to understand algorithms beyond theory\nSharing knowledge: Helping fellow learners see practical workflows and honest reflections\nContinuous growth: Documenting failures and successes as part of the learning process"
  },
  {
    "objectID": "about.html#what-youll-find-here",
    "href": "about.html#what-youll-find-here",
    "title": "About Me",
    "section": "What You’ll Find Here",
    "text": "What You’ll Find Here\nThis isn’t a tutorial site — it’s a documentation of my learning process. Each post represents a step in my understanding of machine learning, from data preprocessing to model evaluation and interpretation.\nCurrent Focus Areas:\n\nMedical data analysis and prediction\nClassical machine learning algorithms\nFeature engineering and selection\nModel interpretation and validation"
  },
  {
    "objectID": "about.html#my-approach",
    "href": "about.html#my-approach",
    "title": "About Me",
    "section": "My Approach",
    "text": "My Approach\nI focus on:\n\nPractical projects with real-world datasets\nClear explanations of methods and reasoning\nVisual storytelling through charts and analysis\nReproducible code that others can learn from\nHonest documentation of challenges and insights"
  },
  {
    "objectID": "about.html#start-here",
    "href": "about.html#start-here",
    "title": "About Me",
    "section": "Start Here",
    "text": "Start Here\nYou can start here or explore the Blog! Have fun!"
  },
  {
    "objectID": "about.html#lets-connect",
    "href": "about.html#lets-connect",
    "title": "About Me",
    "section": "Let’s Connect",
    "text": "Let’s Connect\nI’d love to connect with fellow machine learning enthusiasts, especially those interested in healthcare applications:\n\nLinkedIn: My LinkedIn Profile\nGitHub: My GitHub Profile\n\n\nLearning in public. Bridging medicine and machine learning. Sharing the journey one project at a time."
  },
  {
    "objectID": "home.html",
    "href": "home.html",
    "title": "Intelligent Machines",
    "section": "",
    "text": "Getting Started\n\n\n\nCheck out my latest projects in the Blog section, where I share complete workflows, code, and insights from working with real datasets.\n\n\n\nWelcome to my learning-in-public journal where I document my journey through applied machine learning. Here, you’ll find more information about this blog.\n\nLearning in public • Building understanding • Sharing insights"
  },
  {
    "objectID": "home.html#recent-posts",
    "href": "home.html#recent-posts",
    "title": "Intelligent Machines",
    "section": "",
    "text": "Check out my latest projects in the Blog section, where I share complete workflows, code, and insights from working with real datasets."
  },
  {
    "objectID": "home.html#exploring-machine-learning-through-real-data",
    "href": "home.html#exploring-machine-learning-through-real-data",
    "title": "Intelligent Machines",
    "section": "",
    "text": "Welcome to my learning-in-public journal where I document my journey through applied machine learning. Here, you’ll find more information about this blog.\n\nLearning in public • Building understanding • Sharing insights"
  },
  {
    "objectID": "posts/01_Welcome/Hello_Blog.html",
    "href": "posts/01_Welcome/Hello_Blog.html",
    "title": "Hello Blog!",
    "section": "",
    "text": "Hello and Welcome to My Blog!\nI decided to start this blog to document my machine learning journey in public.\nI am fascinated by idea of building intelligent machines that can learn from data and help make better decisions - especially in fields where precision matters and the cost of errors is high.\nI’m a medical student, so the content will often feature medical topics. But beyond that, I’ll explore whatever I find interesting and intriguing at the moment and share what I learn along the way.\n\n\nJust Read Along\nNot everyone wants — or needs — to write code, and that’s perfectly fine.\nYou can still enjoy this blog by reading along, exploring the visuals, and following my explanations.\nThe goal is to make complex ideas intuitive, whether you’re coding beside me or just curious about how it all works.\n\n\nHow You Can Code Along\nIf you’d like to try the code yourself, you can download my notebooks and run the directly in Google Colab or on Kaggle — no installation required!\nOf course, you can also type along without downloading the notebooks.\nI highly encourage you to experiment with the code — this is one of the best learning experiences you’ll have!\nIf you get stuck or have any questions, feel free to reach out to my via the Contact page or ask an LLM :).\nIn general, ChatGPT, Claude, Gemnini, and similar models are great at explaining code, so feel free to use them to enhance your learning experience!\nThat said, there’s no pressure to code along. You can absolutely enjoy this blog by simply reading the posts, exploring the visuals, and following the explanations.\nEven without writing code, you’ll still get insights into how machine learning works in practice.\n\n\nProgramming Language and Libraries\nThe code will be written in Python, a programming language that’s been around since 1991.\nPython is great for data science, statistics, automation, machine learning and AI. It makes it easy to build prototypes and MVPs quickly. It is also fairly easy to learn, so I encourage you to check it out if any of the topics sound interesting.\nCore libraries I’ll use:\n\nNumpy — fundamental package for scientific computing\n\nPandas — data analysis and manipulation of tabular data\n\nMatplotlib.pyplot — plotting figures and creating visualizations\n\nSeaborn — advanced statistical plotting\nScikit-Learn — classical machine learning algorithms\n\n\n\nThanks for Reading This First Post!\nIf you’d like to know more about me and why I started this blog, check out the About page. Otherwise, dive into the next post and have fun!\nThis is just the beginning. Let’s see where curiosity — and a bit of code — takes us.\n\n\nCredits\nP.S. The image at the beginning of this post was inspired by @EdDonner’s fantastic course on AI Agents!\nWhile the style was inspired by the course, I created the final image myself using an AI model based on my own instructions."
  }
]